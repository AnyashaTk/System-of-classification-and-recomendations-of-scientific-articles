text,space_num,symbolic_percent,space_percent,num_world,num_columns,front_spaces_perc,back_spaces
                                                      Implementing CUDA Streams into AstroAccelerate – A Case Study,54.0,0.4608695652173913,0.5391304347826087,9.0,1.0,0.27835051546391754,0.4072164948453608
"                                                                         Jan Novotný1,3 , Karel Adámek1,2 , and Wes Armour1",73.0,0.352,0.648,9.0,1.0,0.37628865979381443,0.35567010309278346
                                                  1,50.0,0.0196078431372549,0.9803921568627451,1.0,1.0,0.25773195876288657,0.7371134020618557
"                                                  Oxford e-Research Centre, Department of Engineering Science, University of Oxford, 7 Keble Rd,",50.0,0.5694444444444444,0.4305555555555556,13.0,1.0,0.25773195876288657,0.25773195876288657
"                                                                                Oxford OX1 3QG, United Kingdom.",80.0,0.24324324324324326,0.7567567567567568,5.0,1.0,0.41237113402061853,0.4278350515463918
                                               2,47.0,0.020833333333333332,0.9791666666666666,1.0,1.0,0.2422680412371134,0.7525773195876289
"                                                 Faculty of Information Technology, Czech Technical University, Thákurova 9, 160 00, Prague, Czech",49.0,0.5850340136054422,0.41496598639455784,13.0,1.0,0.25257731958762886,0.24226804123711343
arXiv:2101.00941v3 [astro-ph.IM] 6 May 2021,0.0,0.9069767441860465,0.09302325581395354,5.0,1.0,0.0,0.7783505154639175
                                                                                             Republic.,93.0,0.08823529411764706,0.9117647058823529,1.0,1.0,0.4793814432989691,0.4742268041237113
                                              3,46.0,0.02127659574468085,0.9787234042553191,1.0,1.0,0.23711340206185566,0.7577319587628866
"                                                Research Centre for Theoretical Physics and Astrophysics, Institute of Physics, Silesian University in",48.0,0.6,0.4,13.0,1.0,0.24742268041237114,0.22680412371134018
"                                                                   Opava, Bezručovo nám. 13, CZ-74601 Opava, Czech Republic.",67.0,0.4126984126984127,0.5873015873015873,8.0,1.0,0.34536082474226804,0.35051546391752575
"                                                                                                May 7, 2021",96.0,0.08411214953271028,0.9158878504672897,3.0,1.0,0.4948453608247423,0.44845360824742264
"                                              Abstract                                                       using the power of GPUs, which provide excellent en-",46.0,0.32298136645962733,0.6770186335403727,10.0,28.0,0.23711340206185566,0.1701030927835051
"                                                                                                             ergy efficiency [Mittal and Vetter, 2015, Cebri’n et al.,",109.0,0.29518072289156627,0.7048192771084337,9.0,1.0,0.5618556701030928,0.14432989690721654
                                              To be able to run tasks asynchronously on NVIDIA               2012]. To efficiently use such a system we need to,46.0,0.5094339622641509,0.49056603773584906,19.0,8.0,0.23711340206185566,0.18041237113402064
                                              GPUs a programmer must explicitly implement asyn-              ensure that the applications ideally execute functions,46.0,0.558282208588957,0.44171779141104295,14.0,8.0,0.23711340206185566,0.15979381443298968
                                              chronous execution in their code using the syntax of           (kernels) concurrently and the data transfers are hid-,46.0,0.558282208588957,0.44171779141104295,17.0,6.0,0.23711340206185566,0.15979381443298968
"                                              CUDA streams. Streams allow a programmer to launch             den by computations. Beginning with CUDA 7, we",46.0,0.5290322580645161,0.47096774193548385,16.0,7.0,0.23711340206185566,0.2010309278350515
"                                              independent concurrent execution tasks, providing the          can manage this asynchronous behaviour by introduc-",46.0,0.58125,0.41874999999999996,13.0,6.0,0.23711340206185566,0.17525773195876293
                                              ability to utilise different functional units on the GPU       ing “streams”. One of the simplest guides to using this,46.0,0.573170731707317,0.426829268292683,19.0,4.0,0.23711340206185566,0.15463917525773196
"                                              asynchronously. For example, it is possible to transfer        functionality is provided by Harris [2012, 2015].",46.0,0.5759493670886076,0.42405063291139244,15.0,5.0,0.23711340206185566,0.18556701030927836
                                              the results from a previous computation performed on              In this paper we study the implementation of streams,46.0,0.5426829268292683,0.45731707317073167,17.0,8.0,0.23711340206185566,0.15463917525773196
"                                              input data n-1, over the PCIe bus whilst computing             into the AstroAccelerate (AA) project. AA is a GPU-",46.0,0.53125,0.46875,18.0,7.0,0.23711340206185566,0.17525773195876293
"                                              the result for input data n, by placing different tasks        enabled software package that focuses on enabling real-",46.0,0.573170731707317,0.426829268292683,18.0,5.0,0.23711340206185566,0.15463917525773196
                                              in different CUDA streams. The benefit of such an              time processing of time-domain radio-astronomy data.,46.0,0.546583850931677,0.45341614906832295,15.0,8.0,0.23711340206185566,0.1701030927835051
                                              approach is that the time taken for the data transfer          It uses the CUDA programming language for NVIDIA,46.0,0.5414012738853503,0.4585987261146497,18.0,6.0,0.23711340206185566,0.19072164948453607
"                                              between the host (server hosting the GPU) and device           GPUs and can perform tasks such as dedispersion,",46.0,0.5414012738853503,0.4585987261146497,17.0,6.0,0.23711340206185566,0.19072164948453607
"                                              (the GPU) can be hidden with computation. This case            single pulse searching [Adámek and Armour, 2020]",46.0,0.5443037974683544,0.45569620253164556,16.0,7.0,0.23711340206185566,0.18556701030927836
                                              study deals with the implementation of CUDA streams            and Fourier Domain Acceleration Searches (FDAS),46.0,0.5512820512820513,0.4487179487179487,14.0,7.0,0.23711340206185566,0.19587628865979378
"                                              into AstroAccelerate. AstroAccelerate is a GPU ac-             [Dimoudi et al., 2018, Adámek et al., 2017] in real time",46.0,0.5481927710843374,0.4518072289156626,18.0,7.0,0.23711340206185566,0.14432989690721654
                                              celerated real-time signal processing pipeline for time-       on very large data-sets which are comparable to those,46.0,0.5864197530864198,0.4135802469135802,16.0,4.0,0.23711340206185566,0.1649484536082474
                                              domain radio astronomy.                                        which will be produced by next generation radio-,46.0,0.39490445859872614,0.6050955414012739,11.0,21.0,0.23711340206185566,0.19072164948453607
                                                                                                             telescopes such as the Square Kilometre Array (SKA).,109.0,0.2795031055900621,0.7204968944099379,8.0,1.0,0.5618556701030928,0.1701030927835051
                                              1       Introduction                                             The AA code can be divided into few main parts,46.0,0.3184713375796178,0.6815286624203822,12.0,26.0,0.23711340206185566,0.19072164948453607
                                                                                                             as show in Fig. 1. The first part performs the prepa-,109.0,0.2654320987654321,0.7345679012345678,11.0,1.0,0.5618556701030928,0.1649484536082474
                                              With new coming technological instruments in all fields        ration of system and reading user data. The second,46.0,0.5660377358490566,0.4339622641509434,17.0,5.0,0.23711340206185566,0.18041237113402064
                                              of science the need to improve computational algo-             part consists of mapping tasks to suitable resources,46.0,0.546583850931677,0.45341614906832295,16.0,7.0,0.23711340206185566,0.1701030927835051
"                                              rithms, fully utilise hardware architectures, improve          and allocation of all necessary memory. The third part,",46.0,0.5792682926829268,0.4207317073170732,15.0,6.0,0.23711340206185566,0.15463917525773196
"                                              softwares, and compete in upcoming data challenges             which is the part of the code in which we have imple-",46.0,0.5308641975308642,0.4691358024691358,19.0,7.0,0.23711340206185566,0.1649484536082474
"                                              [Geist and Lucas, 2009], is becoming ever more impor-          mented streams, is responsible for the dedispersion of",46.0,0.5644171779141104,0.4355828220858896,17.0,6.0,0.23711340206185566,0.15979381443298968
"                                              tant. Today, by looking to the worlds most powerful            data and single pulse searching. The fourth part offers",46.0,0.5487804878048781,0.4512195121951219,18.0,7.0,0.23711340206185566,0.15463917525773196
"                                              machines [TOP500.org, a,b], we can be certain that to          optional features like FDAS (Fourier Domain Acceler-",46.0,0.5652173913043478,0.4347826086956522,16.0,6.0,0.23711340206185566,0.1701030927835051
"                                              reach exaFLOPS performance, heterogeneous comput-              ation Search) or periodicity searching.",46.0,0.5405405405405406,0.45945945945945943,10.0,8.0,0.23711340206185566,0.23711340206185572
                                              ing is necessary. Clear leaders in this area are systems         To achieve the desired asynchronous behaviour (as,46.0,0.5625,0.4375,17.0,5.0,0.23711340206185566,0.17525773195876293
                                                                                                         1,105.0,0.009433962264150943,0.9905660377358491,1.0,1.0,0.5412371134020618,0.45360824742268047
avr_spaces,53.59574468085106,1.0,0.0,1.0,1.0,0.2762667251590261,0.9484536082474226
                                                                                                                                    pulse search.1 For the correctness of the streams im-,132.0,0.24324324324324326,0.7567567567567568,9.0,1.0,0.6804123711340206,0.046391752577319534
                                                           t_Chunk_0,59.0,0.1323529411764706,0.8676470588235294,1.0,1.0,0.30412371134020616,0.6494845360824743
                                                                                                                                    plementation we use visual inspection of the timeline,132.0,0.24864864864864866,0.7513513513513513,8.0,1.0,0.6804123711340206,0.046391752577319534
                           Preparation,27.0,0.2894736842105263,0.7105263157894737,1.0,1.0,0.13917525773195877,0.8041237113402062
"                                                                                                Periodicity, FDAS?",96.0,0.14912280701754385,0.8508771929824561,2.0,1.0,0.4948453608247423,0.41237113402061853
                                                           DDTR         SPDT,59.0,0.10526315789473684,0.8947368421052632,2.0,5.0,0.30412371134020616,0.6082474226804124
                                                                                                                                    in NVidia Visual Profiler (nvvp). All testing is done,132.0,0.24324324324324326,0.7567567567567568,9.0,1.0,0.6804123711340206,0.046391752577319534
     Initialization                            Computing,5.0,0.4107142857142857,0.5892857142857143,2.0,15.0,0.02577319587628866,0.711340206185567
                                                           t_Chunk_1,59.0,0.1323529411764706,0.8676470588235294,1.0,1.0,0.30412371134020616,0.6494845360824743
                                                                                                                                    on a Tesla V100 GPU.,132.0,0.10526315789473684,0.8947368421052632,5.0,1.0,0.6804123711340206,0.21649484536082475
                                                           DDTR         SPDT,59.0,0.10526315789473684,0.8947368421052632,2.0,5.0,0.30412371134020616,0.6082474226804124
                           memory allocation,27.0,0.36363636363636365,0.6363636363636364,2.0,1.0,0.13917525773195877,0.7731958762886598
                                                                  ...                                                               2.1    Dedispersion phase,66.0,0.1464968152866242,0.8535031847133758,4.0,34.0,0.3402061855670103,0.19072164948453607
                                                           t_Chunk_n,59.0,0.1323529411764706,0.8676470588235294,1.0,1.0,0.30412371134020616,0.6494845360824743
"                                                           DDTR         SPDT                                                        The host memory is pageable by default, which means",59.0,0.2786885245901639,0.7213114754098361,11.0,33.0,0.30412371134020616,0.05670103092783507
                                                                                                                                    that the GPU cannot address the data directly. To,132.0,0.2265193370165746,0.7734806629834254,9.0,1.0,0.6804123711340206,0.0670103092783505
Figure 1: Simplified schema of the AstroAccelerate                                                                                  be able to overlap kernel execution and data trans-,0.0,0.47540983606557374,0.5245901639344263,16.0,42.0,0.0,0.05670103092783507
workflow.                                                                                                                           fers the host memory involved must be pinned –,0.0,0.2640449438202247,0.7359550561797753,10.0,62.0,0.0,0.08247422680412375
                                                                                                                                    in CUDA cudaMallocHost() or cudaHostAlloc() is,132.0,0.2303370786516854,0.7696629213483146,6.0,1.0,0.6804123711340206,0.08247422680412375
"                                                                                                                                    used to allocate pinned memory, to deallocate use",132.0,0.23204419889502761,0.7679558011049724,8.0,1.0,0.6804123711340206,0.0670103092783505
                                                                                                                                    cudaFreeHost().,132.0,0.10204081632653061,0.8979591836734694,1.0,1.0,0.6804123711340206,0.24226804123711343
"shown in Fig. 2, bottom right) of data transfers and                                                                                   When applying the above mentioned points we find",0.0,0.45901639344262296,0.540983606557377,18.0,42.0,0.0,0.05670103092783507
"computing, we split the input signal to n time chunks                                                                               that an increase in the throughput of the memory",0.0,0.4666666666666667,0.5333333333333333,19.0,40.0,0.0,0.07216494845360821
"(these chunks represent the amount of signal that can                                                                               transfers by ∼30 % can be achieved, along with the",0.0,0.4725274725274725,0.5274725274725275,19.0,40.0,0.0,0.061855670103092786
"fit to GPU memory), and again divide them by the                                                                                    benefit of partially overlapping kernels execution.2 An-",0.0,0.4734042553191489,0.5265957446808511,17.0,43.0,0.0,0.030927835051546393
number of desired CUDA streams into smaller chunks.                                                                                 other benefit we obtain is the removal of timing gaps,0.0,0.4756756756756757,0.5243243243243243,18.0,41.0,0.0,0.046391752577319534
These smaller chunks are then associated with a stream                                                                              between copies from device to host. The gaps are,0.0,0.4777777777777778,0.5222222222222221,18.0,40.0,0.0,0.07216494845360821
ID. This process is repeated for all time chunks un-                                                                                caused by the fact that when the copy is invoked the,0.0,0.46195652173913043,0.5380434782608696,21.0,41.0,0.0,0.05154639175257736
til all data are processed. Care has to be taken to                                                                                 driver must allocate a temporary page-locked (pinned),0.0,0.4756756756756757,0.5243243243243243,18.0,41.0,0.0,0.046391752577319534
distribute the correct chunk of memory to the correct                                                                               host array and transfer the data there (see Fig. 3). To,0.0,0.48128342245989303,0.518716577540107,20.0,40.0,0.0,0.03608247422680411
CUDA stream.                                                                                                                        be precise the time saved is just moved to the alloca-,0.0,0.2956989247311828,0.7043010752688172,13.0,61.0,0.0,0.04123711340206182
                                                                                                                                    tion and deallocation of memory where we see signifi-,132.0,0.24324324324324326,0.7567567567567568,9.0,1.0,0.6804123711340206,0.046391752577319534
         Available free memory                                                 Available free memory,9.0,0.38,0.62,6.0,25.0,0.04639175257731959,0.48453608247422686
                                                                                                                                    cant increase (note the increase will be even higher for,132.0,0.25,0.75,10.0,1.0,0.6804123711340206,0.030927835051546393
                                                                          t_chunk_y0                           t_chunk_y1,74.0,0.1652892561983471,0.8347107438016529,2.0,14.0,0.38144329896907214,0.37628865979381443
                      t_chunk_x                                                                                                     systems with larger host memory).,22.0,0.23030303030303031,0.7696969696969697,6.0,51.0,0.1134020618556701,0.14948453608247425
                      default stream 0                                         stream 1                              stream 2,22.0,0.224,0.776,7.0,36.0,0.1134020618556701,0.35567010309278346
                                                                          H2D      t_chunk_y0            D2H,74.0,0.14814814814814814,0.8518518518518519,3.0,10.0,0.38144329896907214,0.44329896907216493
    H2D                 t_chunk_x              D2H,4.0,0.3,0.7,3.0,16.0,0.020618556701030927,0.7422680412371134
                                                                                    H2D     t_chunk_y1                   D2H,84.0,0.12903225806451613,0.8709677419354839,3.0,12.0,0.4329896907216495,0.3608247422680413
Figure 2: Schematic idea of 5the implementation of,0.0,0.86,0.14,8.0,1.0,0.0,0.7422680412371134
CUDA streams into AA.,0.0,0.8571428571428571,0.1428571428571429,4.0,1.0,0.0,0.8917525773195876
                                                                                                                                    Figure 3: Timeline from nvvp showing the default,132.0,0.22777777777777777,0.7722222222222223,8.0,1.0,0.6804123711340206,0.07216494845360821
                                                                                                                                    stream (top) launching data transfers and kernels. A,132.0,0.24456521739130435,0.7554347826086957,8.0,1.0,0.6804123711340206,0.05154639175257736
                                                                                                                                    magnified view of the timeline showing the data trans-,132.0,0.24731182795698925,0.7526881720430108,9.0,1.0,0.6804123711340206,0.04123711340206182
2       Implementation                                                                                                              fer gaps when pageable memory is used (bottom).,0.0,0.30726256983240224,0.6927374301675977,10.0,59.0,0.0,0.07731958762886593
To successfully obtain overlapping data transfers and,0.0,0.8867924528301887,0.1132075471698113,7.0,1.0,0.0,0.7268041237113403
                                                                                                                                       To decrease time caused by the alloca-,135.0,0.18497109826589594,0.815028901734104,7.0,1.0,0.6958762886597938,0.10824742268041232
coherent execution of kernels we perform the follow-,0.0,0.8653846153846154,0.13461538461538458,8.0,1.0,0.0,0.731958762886598
                                                                                                                                    tion/deallocation of the host memory we create,132.0,0.2247191011235955,0.7752808988764045,7.0,1.0,0.6804123711340206,0.08247422680412375
ing steps: 1) create CUDA streams; 2) pinning host,0.0,0.84,0.16000000000000003,9.0,1.0,0.0,0.7422680412371134
                                                                                                                                    smaller temporary buffers to move the data from,132.0,0.22346368715083798,0.776536312849162,8.0,1.0,0.6804123711340206,0.07731958762886593
memory; 3) substitute the commands cudaMemcpy to,0.0,0.875,0.125,7.0,1.0,0.0,0.7525773195876289
                                                                                                                                    host (big pageable memory) to host (small pinned,132.0,0.22777777777777777,0.7722222222222223,8.0,1.0,0.6804123711340206,0.07216494845360821
cudaMemcpyAsync; 4) associate streams ID to kernels,0.0,0.8823529411764706,0.11764705882352944,7.0,1.0,0.0,0.7371134020618557
                                                                                                                                    memory). Using this approach significant time can,132.0,0.23756906077348067,0.7624309392265194,7.0,1.0,0.6804123711340206,0.0670103092783505
and memory transfers; 5) appropriately change all,0.0,0.8775510204081632,0.12244897959183676,7.0,1.0,0.0,0.7474226804123711
                                                                                                                                    be saved in the preparation of the host memory.,132.0,0.21787709497206703,0.782122905027933,9.0,1.0,0.6804123711340206,0.07731958762886593
other explicit (wait event commands) and implicit (e.g.,0.0,0.8727272727272727,0.12727272727272732,8.0,1.0,0.0,0.7164948453608248
                                                                                                                                       1,135.0,0.007352941176470588,0.9926470588235294,1.0,1.0,0.6958762886597938,0.2989690721649485
"memory set, memory allocation) synchronisation com-                                                                                      These parts consists of several kernels, however we name",0.0,0.48704663212435234,0.5129533678756477,15.0,44.0,0.0,0.005154639175257714
mands to non-blocking ones.                                                                                                         them by their main meaning.,0.0,0.29559748427672955,0.7044025157232705,9.0,53.0,0.0,0.18041237113402064
                                                                                                                                       2,135.0,0.007352941176470588,0.9926470588235294,1.0,1.0,0.6958762886597938,0.2989690721649485
                                                                                                                                         The concurrency of kernels can happen when the resources,137.0,0.24870466321243523,0.7512953367875648,9.0,1.0,0.7061855670103093,0.005154639175257714
  We divide the implementation into two phases: com-                                                                                of the GPU are not fully utilised. This can be seen usually at,2.0,0.4793814432989691,0.5206185567010309,21.0,41.0,0.010309278350515464,0.0
"putation of the dedispersion data, and then the single                                                                              the end of kernel executions when the resources are released.",0.0,0.5077720207253886,0.49222797927461137,19.0,40.0,0.0,0.005154639175257714
                                                                                                                                2,128.0,0.007751937984496124,0.9922480620155039,1.0,1.0,0.6597938144329897,0.3350515463917526
avr_spaces,57.52941176470589,1.0,0.0,1.0,1.0,0.29654335961188605,0.9484536082474226
"However, the host to host copies block the CUDA 4 Acknowledgements",0.0,0.8484848484848485,0.1515151515151515,11.0,1.0,0.0,0.6597938144329897
streams. A multi-threaded approach can be used to,0.0,0.8571428571428571,0.1428571428571429,8.0,1.0,0.0,0.7474226804123711
"solve this issue, i.e., for every stream ID we create a The authors would like to express their gratitude",0.0,0.8285714285714286,0.17142857142857137,19.0,1.0,0.0,0.4587628865979382
corresponding CPU thread using OpenMP. The final to the Research Centre for Theoretical Physics and,0.0,0.8585858585858586,0.14141414141414144,15.0,1.0,0.0,0.4896907216494846
"achieved coherence and overlapping is shown in Fig. 4. Astrophysics, Institute of Physics, Silesian Univer-",0.0,0.8691588785046729,0.13084112149532712,15.0,1.0,0.0,0.44845360824742264
"                                                          sity in Opava for institutional support, and the",58.0,0.3867924528301887,0.6132075471698113,8.0,1.0,0.29896907216494845,0.45360824742268047
                                                          support of the OP VVV MEYS funded project,58.0,0.3434343434343434,0.6565656565656566,8.0,1.0,0.29896907216494845,0.4896907216494846
                                                          CZ.02.1.01/0.0/0.0/16 019/0000765 ”Research Center,58.0,0.4351851851851852,0.5648148148148149,4.0,1.0,0.29896907216494845,0.44329896907216493
                                                          for Informatics.,58.0,0.20270270270270271,0.7972972972972973,2.0,1.0,0.29896907216494845,0.6185567010309279
Figure 4: Timeline from nvvp showing the CUDA,0.0,0.8444444444444444,0.15555555555555556,8.0,1.0,0.0,0.768041237113402
streams working in AA.,0.0,0.8636363636363636,0.13636363636363635,4.0,1.0,0.0,0.8865979381443299
                                                          References,58.0,0.14705882352941177,0.8529411764705882,1.0,1.0,0.29896907216494845,0.6494845360824743
                                                          Al Geist and Robert Lucas.                     Major computer,58.0,0.29411764705882354,0.7058823529411764,7.0,11.0,0.29896907216494845,0.38659793814432986
                                                             science challenges at exascale.                    The Inter-,61.0,0.30327868852459017,0.6967213114754098,6.0,11.0,0.31443298969072164,0.3711340206185567
2.2 Single pulse search phase                                national Journal of High Performance Com-,0.0,0.5980392156862745,0.4019607843137255,11.0,17.0,0.0,0.4742268041237113
"                                                             puting     Applications,           23(4):427–436,       2009.",61.0,0.3114754098360656,0.6885245901639344,4.0,11.0,0.31443298969072164,0.3711340206185567
This part of the code deals with single pulse search.        doi:   1 0 . 1 1 7 7 / 1 0 9 4 3 4 2 0 0 9 3 4 7 4 45.   URL,0.0,0.628099173553719,0.371900826446281,35.0,7.0,0.0,0.37628865979381443
In Fig. 1 this step is designated as SPDT. As dynamic        https://doi.org/10.1177/1094342009347445.,0.0,0.8235294117647058,0.17647058823529416,12.0,5.0,0.0,0.4742268041237113
"allocation and deallocation of memory is used in this TOP500.org.               Home – | TOP500, a.                   URL",0.0,0.6115702479338843,0.38842975206611574,16.0,17.0,0.0,0.37628865979381443
"step which gives rise to synchronous behaviour, we           https://www.top500.org/.",0.0,0.788235294117647,0.21176470588235297,9.0,6.0,0.0,0.5618556701030928
have moved memory allocations from the Computation,0.0,0.88,0.12,7.0,1.0,0.0,0.7422680412371134
"phase to the Preparation/Memory allocation phase of TOP500.org.               Green500 – | TOP500, b.                 URL",0.0,0.6446280991735537,0.3553719008264463,14.0,16.0,0.0,0.37628865979381443
the code. The final change required was to change or         https://www.top500.org/lists/green500/.,0.0,0.82,0.18000000000000005,11.0,5.0,0.0,0.48453608247422686
remove the explicit synchronisation events to stream ID,0.0,0.8727272727272727,0.12727272727272732,8.0,1.0,0.0,0.7164948453608248
synchronisation events with their appropriate stream Sparsh Mittal and Jeffrey S. Vetter.                         A survey,0.0,0.6967213114754098,0.30327868852459017,14.0,13.0,0.0,0.3711340206185567
IDs.                                                         of cpu-gpu heterogeneous computing techniques.,0.0,0.42990654205607476,0.5700934579439252,6.0,29.0,0.0,0.44845360824742264
"                                                             ACM Comput. Surv., 47(4), July 2015.                     ISSN",61.0,0.28688524590163933,0.7131147540983607,7.0,11.0,0.31443298969072164,0.3711340206185567
   During the implementation of CUDA streams in this,3.0,0.8076923076923077,0.1923076923076923,8.0,1.0,0.015463917525773196,0.731958762886598
                                                             0360-0300.       doi: 1 0 . 1 1 4 5 / 2 7 8 8 3 9 6.     URL,61.0,0.2727272727272727,0.7272727272727273,18.0,6.0,0.31443298969072164,0.37628865979381443
phase several problems were encountered. The most,0.0,0.8775510204081632,0.12244897959183676,7.0,1.0,0.0,0.7474226804123711
                                                             https://doi.org/10.1145/2788396.,61.0,0.34408602150537637,0.6559139784946236,1.0,1.0,0.31443298969072164,0.5206185567010309
crucial is the fact that the cudaStreamWaitEvent(),0.0,0.88,0.12,7.0,1.0,0.0,0.7422680412371134
"does not work properly when atomic operations are J. M. Cebri’n, G. D. Guerrero, and J. M. Garcı́a.",0.0,0.8282828282828283,0.1717171717171717,18.0,1.0,0.0,0.4896907216494846
used. This forced us to leave the implementation of          Energy efficiency analysis of gpus. In 2012 IEEE,0.0,0.7706422018348624,0.22935779816513757,17.0,6.0,0.0,0.4381443298969072
CUDA streams in this phase for future work.                  26th International Parallel and Distributed Process-,0.0,0.7345132743362832,0.2654867256637168,14.0,10.0,0.0,0.41752577319587625
"                                                             ing Symposium Workshops PhD Forum, pages 1014–",61.0,0.37383177570093457,0.6261682242990654,7.0,1.0,0.31443298969072164,0.44845360824742264
"                                                             1022, 2012. doi: 10.1109/IPDPSW.2012.124.",61.0,0.37254901960784315,0.6274509803921569,4.0,1.0,0.31443298969072164,0.4742268041237113
3     Conclusion                                          Mark Harris. How to Overlap Data Transfers in CUDA,0.0,0.49074074074074076,0.5092592592592593,11.0,24.0,0.0,0.44329896907216493
"                                                             C/C++, December 2012.",61.0,0.23170731707317074,0.7682926829268293,3.0,1.0,0.31443298969072164,0.5773195876288659
Introducing CUDA streams into sophisticated software,0.0,0.9038461538461539,0.09615384615384615,6.0,1.0,0.0,0.731958762886598
like AstroAccelerate was not a straightforward job. We Mark Harris.                 GPU Pro Tip:                  CUDA 7,0.0,0.6083333333333333,0.3916666666666667,15.0,18.0,0.0,0.38144329896907214
"have run into several issues such as stream event bar-       Streams Simplify Concurrency, 2015.                      URL",0.0,0.6611570247933884,0.33884297520661155,15.0,15.0,0.0,0.37628865979381443
riers not working with atomic operations or significant      https://developer.nvidia.com/blog/how-overlap-data,0.0,0.8828828828828829,0.11711711711711714,9.0,4.0,0.0,0.4278350515463918
increases in allocation time for pinned host memory. Karel Adámek and Wesley Armour.                               Single-,0.0,0.6504065040650406,0.34959349593495936,14.0,16.0,0.0,0.365979381443299
As a result CUDA streams are used only in the dedis-         pulse detection algorithms for real-time fast ra-,0.0,0.7727272727272727,0.2272727272727273,18.0,5.0,0.0,0.4329896907216495
persion phase of our code. We have tested the AA             dio burst searches using gpus. The Astrophys-,0.0,0.7358490566037735,0.26415094339622647,17.0,7.0,0.0,0.45360824742268047
"stream version of our code on an SKA-like input signal       ical Journal Supplement Series, 247(2):56, 2020.",0.0,0.8073394495412844,0.19266055045871555,16.0,4.0,0.0,0.4381443298969072
"(1400 MHz central frequency, sampling rate 64 µs, 4096       doi: 1 0 . 3 8 4 7 / 1 5 3 8 - 4 3 6 5 / a b 7 9 94.     URL",0.0,0.6446280991735537,0.3553719008264463,34.0,6.0,0.0,0.37628865979381443
"channels, 300 MHz bandwidth) and achieved an over-           https://doi.org/10.3847/1538-4365/ab7994.",0.0,0.8235294117647058,0.17647058823529416,9.0,6.0,0.0,0.4742268041237113
all speedup of 1.33 against the non-streams version of,0.0,0.8518518518518519,0.14814814814814814,9.0,1.0,0.0,0.7216494845360825
"AA with a dispersion measure plan computing ∼6000 Sofia Dimoudi, Karel Adamek, Prabu Thiagaraj,",0.0,0.8631578947368421,0.13684210526315788,14.0,1.0,0.0,0.5103092783505154
"trials from 0–3000 pc cm−3 .                                 Scott M Ransom, Aris Karastergiou, and Wesley",0.0,0.5849056603773585,0.41509433962264153,13.0,17.0,0.0,0.45360824742268047
                                                        3,56.0,0.017543859649122806,0.9824561403508771,1.0,1.0,0.28865979381443296,0.7061855670103092
avr_spaces,16.574074074074073,1.0,0.0,1.0,1.0,0.08543337151584574,0.9484536082474226
 Armour. A gpu implementation of the correlation,1.0,0.8541666666666666,0.14583333333333337,7.0,1.0,0.005154639175257732,0.7525773195876289
 technique for real-time fourier domain pulsar accel-,1.0,0.8679245283018868,0.13207547169811318,7.0,1.0,0.005154639175257732,0.7268041237113403
 eration searches. The Astrophysical Journal Supple-,1.0,0.8846153846153846,0.11538461538461542,6.0,1.0,0.005154639175257732,0.731958762886598
" ment Series, 239(2):28, 2018. doi: 10.3847/1538-436",1.0,0.8846153846153846,0.11538461538461542,6.0,1.0,0.005154639175257732,0.731958762886598
 5/aabe88.,1.0,0.9,0.09999999999999998,1.0,1.0,0.005154639175257732,0.9484536082474226
"Karel Adámek, Sofia Dimoudi, Mike Giles, and Wesley",0.0,0.8653846153846154,0.13461538461538458,8.0,1.0,0.0,0.731958762886598
 Armour. Improved acceleration of the gpu fourier do-,1.0,0.8490566037735849,0.15094339622641506,8.0,1.0,0.005154639175257732,0.7268041237113403
 main acceleration search algorithm. arXiv preprint,1.0,0.8823529411764706,0.11764705882352944,6.0,1.0,0.005154639175257732,0.7371134020618557
" arXiv:1711.10855, 2017.",1.0,0.9166666666666666,0.08333333333333337,2.0,1.0,0.005154639175257732,0.8762886597938144
                                                      4,54.0,0.01818181818181818,0.9818181818181818,1.0,1.0,0.27835051546391754,0.7164948453608248
avr_spaces,5.636363636363637,1.0,0.0,1.0,1.0,0.029053420805998126,0.9484536082474226
